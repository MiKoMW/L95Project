The old car broke down in the car park.
At least two men broke in and stole my TV.
The horses were broken in and ridden in two weeks.
It was my aunt’s car which we sold at auction last year in February.
The only rabbit that I ever liked was eaten by my parents one summer.
English also has many words of more or less unique function, including interjections (oh, ah), negatives (no, not), politeness markers (please, thank you), and the existential ‘there’(there are horses but not unicorns) among others.
Making these decisions requires sophisticated knowledge of syntax; tagging manuals (Santorini,1990) give various heuristics that can help human coders make these decisions and that can also provide useful features for automatic taggers.
The Penn Treebank tagset was culled from the original 87-tag tagset for the Brown Corpus. For example the original Brown and C5 tagsets include a separate tag for each of the different forms of the verbs do (e.g. C5 tag VDD for did and VDG tag for doing), be and have.
The slightly simplified version of the Viterbi algorithm that we present takes as input a single HMM and a sequence of observed words O = (o1,o2, ...oT ) and returns the most probable state/tag sequence Q = (q1,q2,qT ) together with its probability.
Thus the EM-trained “pure HMM” tagger is probably best suited to cases where no training data is available, for example, when tagging languages for which no data was previously hand-tagged.